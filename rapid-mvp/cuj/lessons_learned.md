# Lessons Learned — Team Postmortem on Classroom Presentation Randomizer MVP

Our MVP met the functional requirements: the randomizer selects teams with no repeats, the timer transitions from presentation to Q&A, and the 2-minute warning triggers automatically. If we had stopped there, we would have declared the product “done.” The CUJ audit changed that definition by making us quantify what actually costs time and attention during a live session.

In our Unhappy Path audit, the total time to reach a full first presentation cycle was dominated by setup rather than execution (~5–7 minutes before the first team fully completes presentation + Q&A). We also logged **2 context switches** that required user's window shifts and mental gear shifts. Firstly, switching taps to obtain team names for manually inputting 12 teams, and secondly entering a rework loop when we discovered a typo and had to delete and re-add the team (no edit-in-place flow). Those two moments were not “bugs,” but they were real workflow interrupts that increased cognitive load and introduced avoidable errors.

The audit surfaced how “correct” functionality may not be sufficient to be fully "usable” under pressure. Our biggest surprise came from the timer inputs. The timer logic worked as intended, yet the input UI created hesitation and confusion: keyboard delete could leave an undeletable `0`, and the display formatting could produce values like `01` or `010`. This isn’t a backend failure—it’s a usability failure that costs real time (15–30 seconds) at exactly the moment the instructor is trying to keep class pacing tight. In other words, we optimized backend correctness but didn’t test the input component under realistic typing patterns, and the CUJ audit made that visible.  Similarly, even though the randomizer correctly prevents repeats by removing teams from the backend pool, the UI does not explicitly mark which teams have already presented. That gap isn’t a deal-breaker for functionality, but it creates an unpleasant UX where the instructor can’t verify session progress at a glance and has to rely on the remaining counter instead. 

As a result, our definition of “done” shifted. **Before:** done meant endpoints work, timer transitions smoothly, and team selection is random and correct. **After:** done means it’s easy to identify session progress at a glance (how many teams have gone and how many are left), inputs are forgiving under rushed interaction, and the timer system leaves “space” for real classroom transitions (e.g., brief instructor commentary between presentation and Q&A). The auto-switch to Q&A is logically clean, but without a buffer or confirmation, it can silently steal seconds from students when Q&A windows are already short.

 For our next iteration, we will focus on improvements that are not new features but  friction removals: add an explicit **Presented/Completed** visual state so progress is visible without mental bookkeeping, and harden the timer UX with clearable inputs, normalized display, and a transition buffer (e.g., “Start Q&A?” prompt). We optimized backend correctness but didn’t test how these interactions feel under realistic classroom pressure, and the CUJ audit made that visible.