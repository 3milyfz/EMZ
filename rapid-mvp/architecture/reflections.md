# A3 Reflections: From Local Prototype to Production-Grade System with CUJ & Week2 Feedback

A2’s CUJ audit and peer feedback fundamentally reframed how we design our system architecture and define non-functional requirements. In A2, our MVP was *functionally correct*: the randomizer enforced no repeats, the dashboard displayed team metadata, and the dual-phase timer transitioned as designed. However, our CUJ findings made it clear that correctness does not equal usability—especially under classroom pressure. The audit highlighted that instructors operate in a high-tempo workflow where small frictions (input quirks, unclear progress signals, awkward timer transitions) compound into real cognitive load. Peer feedback reinforced that our tool needed to optimize for the *actual instructor loop*—select team → observe presentation → manage time → capture outcomes → move on—rather than for isolated feature demos.

This feedback directly catalyzed our A3 transition from “works on my machine” to a more production-grade system, not only through cloud deployment and automated delivery, but through intentional workflow-driven refinement. Once persistence became real (via Railway backend + volume-backed SQLite and Prisma), we had to decide what data was worth saving. The answer came from the CUJ: instructors don’t just need a selector and a timer—they need a lightweight way to record and retrieve the outcomes of each presentation. This led to our first major A3 refinement: an **Instructor Notes System**. We added a dedicated notes textarea in the UI for capturing feedback, and a “Save Notes” action that persists to the backend. Notes automatically load when a team is selected so instructors can context-switch less and maintain continuity across the session. Because notes are only useful if they can be revisited outside the live session, we integrated them into our **CSV/JSON export** flow. Architecturally, this required a schema evolution: we added a `notes` field to the `Team` model in Prisma and ensured migrations apply reliably at runtime against the mounted `/data` volume. This was a concrete step toward production thinking: persistence isn’t just about storing state—it’s about storing the *right* state.

Second, our A2 CUJ identified a persistent lowlight: the tool did not clearly communicate session progress. Teams were removed from the selection pool, but the dashboard did not visibly represent “what’s live” versus “what’s done,” and editing/deletion workflows were clunky. In A3, we introduced a **Team List with Management** panel that reflects the instructor’s mental model. The list now shows team name, topic, and members at a glance, includes a per-team remove button, and uses visual indicators (highlight + “Live” badge) for the current team. The list is scrollable for large sections and includes a team counter for quick progress checks. This aligns the UI with how instructors track a room, reducing the need for manual bookkeeping.

Third, we addressed a timing failure mode from A2: instructors may speak briefly between presentation and Q&A and accidentally “steal” Q&A time due to awkward transitions. In A3, we refined the timer control by enabling **Auto-Start Q&A** upon explicit transition, removing an unnecessary extra click and reducing the likelihood of the system being in an unintended idle state. Combined with our broader UI improvements (including addressing **dark/light theme accessibility** for bright classrooms), these changes reflect a shift from feature completion to operational reliability.

Overall, A3 represents a synthesis of feedback into both product and infrastructure maturity: we maintained strict FE/BE separation, deployed to cloud services, ensured persistence with volume-backed storage, and refined the system around instructor workflow. The CUJ audit did not simply generate a list of UI tweaks—it changed our engineering priorities. We moved from “the endpoints work” to “the system supports a real user under pressure,” which is the core transition from a local prototype to a production-grade release.
